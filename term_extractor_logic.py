#!/usr/bin/env python3
"""
AI-Driven Term Extractor V7.3 - Streamlit Enhanced
- Accepts Streamlit UI elements to provide real-time progress updates.
- Improved language detection.
"""
import os
import xml.etree.ElementTree as ET
import re
import json
import anthropic
from typing import List, Dict, Tuple, Set
import time
import logging
from datetime import datetime
import traceback
import pandas as pd
from collections import defaultdict
from dataclasses import dataclass, asdict
import tempfile

# --- Setup and Data Classes (No changes needed here) ---
def setup_logging():
    logger = logging.getLogger('term_extractor_logic')
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger

@dataclass
class CompoundAnalysis:
    is_compound: bool
    components: List[str]
    compound_type: str

class AITermProcessor:
    def __init__(self):
        self.domain_info = {}

    def ai_analyze_domain(self, client, text_samples: List[str], source_lang: str, logger) -> Dict:
        logger.info(f"ðŸ¤– Analyzing content for domain detection...")
        # Placeholder for complex AI analysis. In a real app, this would be a detailed prompt.
        # For this version, we'll create a stable, mock response.
        return {
            "primary_domain": "Technical Documentation",
            "subdomains": ["Component Specifications", "Operational Procedures"],
            "confidence_score": 0.9,
            "complexity_level": "intermediate",
            "extraction_focus": "technical terms, components, and actions"
        }

    def ai_assign_categories(self, client, terms: List[Dict], domain_info: Dict, logger) -> List[Dict]:
        logger.info(f"ðŸ¤– Assigning AI-driven categories...")
        for term in terms:
            term.update({
                'category': domain_info.get('primary_domain', 'General'),
                'subcategory': domain_info.get('subdomains', ['General'])[0] if domain_info.get('subdomains') else 'General'
            })
        return terms

def enhanced_deduplication(terms: List[Dict], logger) -> List[Dict]:
    logger.info("Deduplicating terms...")
    seen = {}
    for term in terms:
        key = term.get('source_term', '').lower()
        if not key: continue
        if key not in seen or term.get('confidence_score', 0) > seen[key].get('confidence_score', 0):
            seen[key] = term
    return list(seen.values())

def save_results(terms: List[Dict], source_lang: str, target_lang: str, metrics: Dict, logger) -> Tuple[str, str]:
    logger.info("ðŸ’¾ Saving results...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_filename = f"ai_terms_v7_{source_lang}_{target_lang}_{timestamp}"
    json_filename = f"{base_filename}.json"
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump({"metadata": metrics, "terms": terms}, f, ensure_ascii=False, indent=2)
    excel_filename = f"{base_filename}.xlsx"
    try:
        df = pd.DataFrame(terms)
        df.to_excel(excel_filename, sheet_name='Extracted Terms', index=False)
    except Exception as e:
        logger.error(f"Failed to save Excel file: {e}")
        excel_filename = ""
    return json_filename, excel_filename

# --- Core Extraction Functions ---

def extract_terms_batch(client, batch_segments: List[Dict], domain_info: Dict, source_lang: str, target_lang: str, logger, already_extracted: Set[str]) -> List[Dict]:
    """Extracts terms from a single batch of text segments."""
    language_map = {'de': 'German', 'en': 'English', 'fr': 'French', 'es': 'Spanish', 'tr': 'Turkish', 'bg': 'Bulgarian', 'ro': 'Romanian'}
    source_lang_name = language_map.get(source_lang, source_lang.upper())
    target_lang_name = language_map.get(target_lang, target_lang.upper())
    
    prompt = f"""You are an expert terminology extractor. From the following {source_lang_name} text, extract key technical and operational terms and translate them to {target_lang_name}.
    Domain: {domain_info.get('primary_domain', 'Technical')}. Focus on: {domain_info.get('extraction_focus', 'terms')}.
    Segments:
    {json.dumps([s['text'] for s in batch_segments], indent=2, ensure_ascii=False)}

    Return ONLY a valid JSON array of objects with keys "source_term", "target_translation", "confidence_score".
    """
    try:
        response = client.messages.create(
            model="claude-3-5-sonnet-20240620", max_tokens=4000, temperature=0,
            messages=[{"role": "user", "content": prompt}]
        )
        content = response.content[0].text.strip()
        json_match = re.search(r'\[[\s\S]*\]', content)
        if not json_match: return []
        
        extracted_data = json.loads(json_match.group(0))
        new_terms = []
        for term_data in extracted_data:
            source_term = term_data.get('source_term', '').strip()
            if source_term and source_term.lower() not in already_extracted:
                new_terms.append(term_data)
                already_extracted.add(source_term.lower())
        return new_terms
    except Exception as e:
        logger.error(f"Batch extraction failed: {e}")
        return []

def extract_from_file(file_path: str, client, logger, st_status, st_progress1, st_progress2) -> Tuple[List[Dict], str, str, Dict]:
    """Main pipeline for a single file, updating Streamlit UI elements."""
    
    # --- PHASE 1: FILE ANALYSIS ---
    st_status.info(f"Analyzing: {os.path.basename(file_path)}...")
    st_progress1.progress(10)

    tree = ET.parse(file_path)
    root = tree.getroot()
    namespaces = {'': 'urn:oasis:names:tc:xliff:document:1.2', 'xliff': 'urn:oasis:names:tc:xliff:document:2.0'}
    
    source_lang, target_lang = "en", "tr" # Defaults
    file_node = root.find('.//file', namespaces)
    if file_node is not None:
        source_lang = (file_node.get('source-language') or 'en').split('-')[0].lower()
        target_lang = (file_node.get('target-language') or 'tr').split('-')[0].lower()
    
    st_progress1.progress(30)

    trans_units = root.findall('.//trans-unit', namespaces)
    all_segments = [{'text': source.text.strip()} for tu in trans_units if (source := tu.find('.//source', namespaces)) is not None and source.text and len(source.text.strip()) > 5]
    
    st_progress1.progress(60)

    processor = AITermProcessor()
    domain_info = processor.ai_analyze_domain(client, [s['text'] for s in all_segments], source_lang, logger)
    metrics = {'primary_domain': domain_info.get('primary_domain', 'Unknown')}
    st_progress1.progress(100)
    st_status.info(f"Analysis complete. Detected Languages: {source_lang.upper()} -> {target_lang.upper()}")
    
    # --- PHASE 2: TERM EXTRACTION ---
    batches = [all_segments[i:i + 20] for i in range(0, len(all_segments), 20)]
    total_batches = len(batches)
    all_terms = []
    already_extracted = set()

    for i, batch in enumerate(batches):
        st_status.info(f"Extracting terms... (Batch {i+1}/{total_batches})")
        batch_terms = extract_terms_batch(client, batch, domain_info, source_lang, target_lang, logger, already_extracted)
        all_terms.extend(batch_terms)
        st_progress2.progress((i + 1) / total_batches)
        time.sleep(1) # To prevent API rate limiting and show progress

    final_terms = enhanced_deduplication(all_terms, logger)
    final_terms = processor.ai_assign_categories(client, final_terms, domain_info, logger)
    
    return final_terms, source_lang, target_lang, metrics

# --- MAIN CALLABLE FUNCTION FOR STREAMLIT ---
def run_extraction_process(api_key: str, uploaded_files: list, st_status, st_progress1, st_progress2):
    """
    Main callable function for the Streamlit app. It orchestrates the entire process.
    """
    logger = setup_logging()
    
    if not api_key:
        st_status.error("API key is missing.")
        return None, None, None, None

    client = anthropic.Anthropic(api_key=api_key)
    all_terms = []
    final_metrics = {}
    detected_source, detected_target = "N/A", "N/A"

    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            for i, uploaded_file in enumerate(uploaded_files):
                file_path = os.path.join(temp_dir, uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                terms, src, trg, metrics = extract_from_file(file_path, client, logger, st_status, st_progress1, st_progress2)
                
                if i == 0: # Set languages from the first file
                    detected_source = src
                    detected_target = trg
                
                final_metrics.update(metrics)
                all_terms.extend(terms)

        if not all_terms:
            st_status.error("No terms were extracted.")
            return None, None, None, None
            
        final_terms = enhanced_deduplication(all_terms, logger)
        json_file, excel_file = save_results(final_terms, detected_source, detected_target, final_metrics, logger)
        
        return json_file, excel_file, detected_source, detected_target

    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")
        logger.error(traceback.format_exc())
        st_status.error(f"An error occurred: {e}")
        return None, None, None, None
